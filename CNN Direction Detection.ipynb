{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from utils.classifications_utils import *\n",
    "from utils.data_processing_utils import *\n",
    "from utils.data_visualization_utils import *\n",
    "from utils.metrics_utils import *\n",
    "from utils.grid_search_utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify Project and drone data\n",
    "project = \"Project 2\"\n",
    "drone_in_use = \"drone1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_0_wind = load_data(2.5, 6, project, drone_in_use, direction=\"\")\n",
    "level_0_wind = separate_data_based_on_apparatus(level_0_wind)\n",
    "\n",
    "level_1_wind = load_data(2.5, 6, project, drone_in_use, direction=\"away\")\n",
    "level_1_wind = separate_data_based_on_apparatus(level_1_wind)\n",
    "\n",
    "level_2_wind = load_data(2.5, 6, project, drone_in_use, direction=\"left\")\n",
    "level_2_wind = separate_data_based_on_apparatus(level_2_wind)\n",
    "\n",
    "level_3_wind = load_data(2.5, 6, project, drone_in_use, direction=\"right\")\n",
    "level_3_wind = separate_data_based_on_apparatus(level_3_wind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_no_wind, gyro_no_wind, mag, stab = level_0_wind.values()\n",
    "label_0 = [0 for x in range(acc_no_wind.shape[0])]\n",
    "\n",
    "acc_level_1_wind, gyro_level_1_wind, mag, stab = level_1_wind.values()\n",
    "label_1 = [1 for x in range(acc_level_1_wind.shape[0])]\n",
    "\n",
    "acc_level_2_wind, gyro_level_2_wind, mag, stab = level_2_wind.values()\n",
    "label_2 = [2 for x in range(acc_level_2_wind.shape[0])]\n",
    "\n",
    "acc_level_3_wind, gyro_level_3_wind, mag, stab = level_3_wind.values()\n",
    "label_3 = [3 for x in range(acc_level_3_wind.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_gyro_0, X_test_gyro_0, y_train_gyro_0, y_test_gyro_0 = \\\n",
    "train_test_split(gyro_no_wind, label_0, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gyro_1, X_test_gyro_1, y_train_gyro_1, y_test_gyro_1 = \\\n",
    "train_test_split(gyro_level_1_wind, label_1, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gyro_2, X_test_gyro_2, y_train_gyro_2, y_test_gyro_2 = \\\n",
    "train_test_split(gyro_level_2_wind, label_2, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_gyro_3, X_test_gyro_3, y_train_gyro_3, y_test_gyro_3 = \\\n",
    "train_test_split(gyro_level_3_wind, label_3, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_gyro = X_train_gyro_0.append(X_train_gyro_1)\n",
    "X_train_gyro = X_train_gyro.append(X_train_gyro_2)\n",
    "X_train_gyro = X_train_gyro.append(X_train_gyro_3)\n",
    "y_train_gyro = np.hstack((y_train_gyro_0, y_train_gyro_1, y_train_gyro_2, y_train_gyro_3))\n",
    "train_data = X_train_gyro\n",
    "sLength = len(train_data)\n",
    "train_data['labels'] = y_train_gyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_gyro = X_test_gyro_0.append(X_test_gyro_1)\n",
    "X_test_gyro = X_test_gyro.append(X_test_gyro_2)\n",
    "X_test_gyro = X_test_gyro.append(X_test_gyro_3)\n",
    "test_labels = np.hstack((y_test_gyro_0, y_test_gyro_1, y_test_gyro_2, y_test_gyro_3))\n",
    "test_data = X_test_gyro\n",
    "sLength = len(test_data)\n",
    "test_data['labels'] = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data, size):\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += (size / 2)\n",
    "\n",
    "def segment_signal(data,window_size = 100):\n",
    "    segments = np.empty((0,window_size,3))\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data[\"labels\"], window_size):\n",
    "        x = data[\"gyro.x\"][start:end]\n",
    "        y = data[\"gyro.y\"][start:end]\n",
    "        z = data[\"gyro.z\"][start:end]\n",
    "        if(len(data[\"labels\"][start:end]) == window_size):\n",
    "            segments = np.vstack([segments,np.dstack([x,y,z])])\n",
    "            labels = np.append(labels,stats.mode(data[\"labels\"][start:end])[0][0])\n",
    "    return segments, labels\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev = 0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.0, shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def depthwise_conv2d(x, W):\n",
    "    return tf.nn.depthwise_conv2d(x,W, [1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def apply_depthwise_conv(x,kernel_size,num_channels,depth):\n",
    "    weights = weight_variable([1, kernel_size, num_channels, depth])\n",
    "    biases = bias_variable([depth * num_channels])\n",
    "    return tf.nn.relu(tf.add(depthwise_conv2d(x, weights),biases))\n",
    "    \n",
    "def apply_max_pool(x,kernel_size,stride_size):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, kernel_size, 1], \n",
    "                          strides=[1, 1, stride_size, 1], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(train_data)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,100, 3)\n",
    "train_x = reshaped_segments\n",
    "train_y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments, labels = segment_signal(test_data)\n",
    "labels = np.asarray(pd.get_dummies(labels), dtype = np.int8)\n",
    "reshaped_segments = segments.reshape(len(segments), 1,100, 3)\n",
    "test_x = reshaped_segments\n",
    "test_y = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_height = 1\n",
    "input_width = 100\n",
    "num_labels = 4\n",
    "num_channels = 3\n",
    "\n",
    "batch_size = 10\n",
    "kernel_size = 60\n",
    "depth = 60\n",
    "num_hidden = 1000\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 10\n",
    "\n",
    "total_batches = train_x.shape[0] // batch_size\n",
    "display(total_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None,input_height,input_width,num_channels])\n",
    "Y = tf.placeholder(tf.float32, shape=[None,num_labels])\n",
    "\n",
    "c = apply_depthwise_conv(X,kernel_size,num_channels,depth)\n",
    "p = apply_max_pool(c,20,2)\n",
    "c = apply_depthwise_conv(p,6,depth*num_channels,depth//10)\n",
    "\n",
    "shape = c.get_shape().as_list()\n",
    "c_flat = tf.reshape(c, [-1, shape[1] * shape[2] * shape[3]])\n",
    "\n",
    "f_weights_l1 = weight_variable([shape[1] * shape[2] * depth * num_channels * (depth//10), num_hidden])\n",
    "f_biases_l1 = bias_variable([num_hidden])\n",
    "f = tf.nn.tanh(tf.add(tf.matmul(c_flat, f_weights_l1),f_biases_l1))\n",
    "\n",
    "out_weights = weight_variable([num_hidden, num_labels])\n",
    "out_biases = bias_variable([num_labels])\n",
    "y_ = tf.nn.softmax(tf.matmul(f, out_weights) + out_biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -tf.reduce_sum(Y * tf.log(y_))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate).minimize(loss)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_,1), tf.argmax(Y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Training Loss: 3.1471376e-05 Training Accuracy: 0.24967434\n",
      "Epoch: 1 Training Loss: 4.1842683e-05 Training Accuracy: 0.24967434\n",
      "Epoch: 2 Training Loss: 1.740459e-05 Training Accuracy: 0.24967434\n",
      "Epoch: 3 Training Loss: 6.544661e-05 Training Accuracy: 0.24967434\n",
      "Epoch: 4 Training Loss: 0.00047955368 Training Accuracy: 0.24967434\n",
      "Epoch: 5 Training Loss: 0.00021136228 Training Accuracy: 0.24967434\n",
      "Epoch: 6 Training Loss: 0.00017333249 Training Accuracy: 0.24967434\n",
      "Epoch: 7 Training Loss: 0.00012791262 Training Accuracy: 0.24967434\n",
      "Epoch: 8 Training Loss: 0.00026870973 Training Accuracy: 0.24967434\n",
      "Epoch: 9 Training Loss: 0.0004916395 Training Accuracy: 0.24967434\n",
      "Testing Accuracy:0.24869566\n"
     ]
    }
   ],
   "source": [
    "cost_history = np.empty(shape=[1],dtype=float)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(training_epochs):\n",
    "        for b in range(total_batches):    \n",
    "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
    "            batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
    "            batch_y = train_y[offset:(offset + batch_size), :]\n",
    "            _, c = session.run([optimizer, loss],feed_dict={X: batch_x, Y : batch_y})\n",
    "            cost_history = np.append(cost_history,c)\n",
    "        print(\"Epoch: \" + str(epoch) + \" Training Loss: \" + str(c) + \" Training Accuracy: \" +\n",
    "              str(session.run(accuracy, feed_dict={X: train_x, Y: train_y})))\n",
    "    print(\"Testing Accuracy:\" + str(session.run(accuracy, feed_dict={X: test_x, Y: test_y})))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
